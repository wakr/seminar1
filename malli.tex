% --- Template for thesis / report with tktltiki2 class ---
% 
% last updated 2013/02/15 for tkltiki2 v1.02

\documentclass[english]{tktltiki2}

% tktltiki2 automatically loads babel, so you can simply
% give the language parameter (e.g. finnish, swedish, english, british) as
% a parameter for the class: \documentclass[finnish]{tktltiki2}.
% The information on title and abstract is generated automatically depending on
% the language, see below if you need to change any of these manually.
% 
% Class options:
% - grading                 -- Print labels for grading information on the front page.
% - disablelastpagecounter  -- Disables the automatic generation of page number information
%                              in the abstract. See also \numberofpagesinformation{} command below.
%
% The class also respects the following options of article class:
%   10pt, 11pt, 12pt, final, draft, oneside, twoside,
%   openright, openany, onecolumn, twocolumn, leqno, fleqn
%
% The default font size is 11pt. The paper size used is A4, other sizes are not supported.
%
% rubber: module pdftex

% --- General packages ---

\PassOptionsToPackage{hyphens}{url}
\usepackage[hyphens]{url}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{amsfonts,amsmath,amssymb,amsthm,booktabs,color,enumitem,graphicx}
\usepackage[pdftex,hidelinks]{hyperref}
%\usepackage[bottom]{footmisc}

% Automatically set the PDF metadata fields
\makeatletter
\AtBeginDocument{\hypersetup{pdftitle = {\@title}, pdfauthor = {\@author}}}
\makeatother

% --- Language-related settings ---
%
% these should be modified according to your language

% babelbib for non-english bibliography using bibtex
\usepackage[fixlanguage]{babelbib}
\selectbiblanguage{english}

% add bibliography to the table of contents
\usepackage[nottoc]{tocbibind}
% tocbibind renames the bibliography, use the following to change it back
\settocbibname{References}

% --- Theorem environment definitions ---

\newtheorem{lau}{Lause}
\newtheorem{lem}[lau]{Lemma}
\newtheorem{kor}[lau]{Korollaari}

\theoremstyle{definition}
\newtheorem{maar}[lau]{Määritelmä}
\newtheorem{ong}{Ongelma}
\newtheorem{alg}[lau]{Algoritmi}
\newtheorem{esim}[lau]{Esimerkki}

\theoremstyle{remark}
\newtheorem*{huom}{Huomautus}


% --- tktltiki2 options ---
%
% The following commands define the information used to generate title and
% abstract pages. The following entries should be always specified:

\title{Automated Tools for Source Code Plagiarism Detection}
\author{Kristian Wahlroos}
\date{\today}
\level{Seminar report}
\abstract{Plagiarism is a serious problem which can be hard to detect. Especially in massive online courses (MOOC), where students are required to complete programming tasks, there is no other guarantee than trust, that the student has really completed his/her assignment completely by themselves.  Same kind of situation is possible in academic world, where universities held courses that relies on students completing various programming tasks. This setting can lead to cases of plagiarism where the solution for a given exercise is actually copied from a study friend or from the Internet. Also obfuscation is often used by plagiarists to hide their cases of plagiarism. This makes detecting plagiarism even harder and with the sheer number of participant, the plagiarism detection is almost impossible to do as a manual labor.

This paper describes the current solutions for automated plagiarism detection from the source code by doing a literature review. The focus will be on the machine learning aspect and the scenario that plagiarism is reflected to, is a scenario where student is completing programming assignments for a grade. This kind of scenario is very common one among universities, and exists also at University of Helsinki's course '\textit{Introduction to programming}'.
}

% The following can be used to specify keywords and classification of the paper:

\keywords{Machine learning, Source code, Plagiarism, Author identification}

% classification according to ACM Computing Classification System (http://www.acm.org/about/class/)
% This is probably mostly relevant for computer scientists
% uncomment the following; contents of \classification will be printed under the abstract with a title
% "ACM Computing Classification System (CCS):"
% \classification{}

% If the automatic page number counting is not working as desired in your case,
% uncomment the following to manually set the number of pages displayed in the abstract page:
%
% \numberofpagesinformation{16 sivua + 10 sivua liitteissä}
%
% If you are not a computer scientist, you will want to uncomment the following by hand and specify
% your department, faculty and subject by hand:
%
% \faculty{Matemaattis-luonnontieteellinen}
% \department{Tietojenkäsittelytieteen laitos}
% \subject{Tietojenkäsittelytiede}
%
% If you are not from the University of Helsinki, then you will most likely want to set these also:
%
% \university{Helsingin Yliopisto}
% \universitylong{HELSINGIN YLIOPISTO --- HELSINGFORS UNIVERSITET --- UNIVERSITY OF HELSINKI} % displayed on the top of the abstract page
% \city{Helsinki}
%


% 10-15 pages abstract

\begin{document}

% --- Front matter ---

\frontmatter      % roman page numbering for front matter

\maketitle        % title page
\makeabstract     % abstract page

\tableofcontents  % table of contents

% --- Main matter ---

\mainmatter       % clear page, start arabic page numbering

\section{Introduction}

% Write some science here.

%Tell motivation (elevator pitches). "What is the point?". %Outline of the rest of the paper.  10-20 citations. 

Massive online courses or \textit{MOOCs} have gained a lot of popularity in recent years. Their course formats are easy to follow and don't require any live attendance from the students to be able to complete the courses. MOOCs also has a quite flexible schedules to complete them, which makes them very interesting in the eyes of a student. Some MOOCs like edX\footnote{See more at \url{https://www.edx.org/credit}} and Coursera can even offer real credits for the students, if students universities have made a deal with them. From available MOOCs, programming has gained a lot of interest and in the end of 2016, edX even listed three programming courses as the years most popular courses\footnote{Based on \url{http://blog.edx.org/10-most-popular-courses-edx-courses-in-2016}}. The course formats of programming MOOCs work more or less the same way; videos for lectures and returnable programming assignments as tasks. These programming assignments are usually automatically tested and credited for the student, and this makes it very easy for the student to complete programming courses from their home computers whenever they want.

Many MOOCs work by trusting that the student has completed exercises by themselves and this is not really guaranteed in any way. In other words there usually doesn't exists any automatic system for possible plagiarism detection inside these courses. This leaves MOOCs in a rather difficult position, because they offer credits based on one-way trust, and without a final exam with mandatory attendance, one-way trust becomes a quite shallow. For example, source code plagiarism is relatively easy and without automatic tools it's almost impossible to get caught if there are hundreds or even thousands of students. Detecting source code plagiarism is thus basically impossible for the course keepers without any automated tools. 

In this paper tools and methodologies for automated source code plagiarism detection is reviewed. The scope will be focusing on various machine learning techniques, while the reflected scenario is kept to be at MOOC-style courses or academic world where plagiarism could happen within the normal programming course with assignments. Also the term \textit{machine learning technique} is considered as a term, that covers following subcategories: unsupervised learning, supervised learning, data mining and various probabilistic models. Thus it's not limited to only to analyze e.g. labeled data, meaning already known cases of plagiarism in one scenario. 

The structure of the rest of this paper will be following: starting with the background motivation follows the methodology part where techniques for the literature review is described; then comes the results where different techniques and features from various papers is described; analyzing part sums up those findings to find common cases or really differing techniques; finally comes discussion part that reflects the findings on how could these be used at programming related MOOCs.

\subsection{Background}


As there are countless of opportunities in MOOCs to plagiarise source code, there are as equal amount of ways to do it in academic courses. Usually plagiarism in courses is not necessary even intentional but just by-product of course friends doing the same tasks together. For example in \textit{Introduction to programming}\footnote{This years course page at \url{https://2017-ohjelmointi.github.io/}} course organized by University of Helsinki, the course lasts 7 weeks and every week student has to complete programming assignments with Java-language. Every student has same assignments in the same order and they are encouraged to help each others during sessions. This naturally can create problematic cases of group work, which as recurring events are also real plagiarism in a form of source code sharing. One solution in here would be automatically detect those cases and then try to resolve them manually.  

Other types of source code plagiarism are direct plagiarism and obfuscated plagiarism. Direct plagiarism in source codes is basically copy-pasting the code and it's the most simplest form of plagiarism. Obfuscation means that the plagiarist is trying to hide his/her plagiarism by making the code look more like it was made by him/her. In source code, obfuscation could relate to renaming variables and functions or making slight modifications into the plagiarised code e.g. changing the order or making slight modification into the logic of the code.

\section{Methodology}

%How you found the articles. Literature review. Aggravating results. What was the systematic way?. Pick venue (edm e.g.). Give year keywords if used. Another way is snowballing. No judging here. 

The references for this paper were gathered by performing systematic literature review utilizing \textit{Google Scholar}. This was done in a two step manner where first a collection of papers were gathered by searching with specific keywords, then these papers were filtered if their abstract, keywords, title or introduction contains specific terms. 

In the first step, used search terms which required a direct match were \textbf{machine learning}, \textbf{plagiarism} and \textbf{code}. Other terms used were \textbf{authorship} and \textbf{identification} but these didn't require a direct match from the results. Papers were filtered also by year, only considering papers from 2006 onward. This was done because most modern MOOCs like Coursera and Udacity are relatively new, and automated programming assignments haven't existed before. Papers from 2006 onward also have a high probability to consider modern programming languages e.g. Java or Python, which both are heavily used to teach programming in universities\footnote{Based on \url{http://cacm.acm.org/blogs/blog-cacm/176450-python-is-now-the-most-popular-introductory-teaching-language-at-top-u-s-universities/fulltext}}. 

Second step contained a more specific filtering, where only papers that included words \textbf{plagiarism}, \textbf{programming} or \textbf{code} were selected. This was done to get more specific results which focuses more on the scope of this paper. That is, detecting source code plagiarism with machine learning. 

Gathered papers are analyzed together to form the final results of this paper. This is done by looking at the methodologies and results of gathered studies by their accuracies, features extractions and usage of machine learning techniques. This is believed to help to give answers on how plagiarism could be detected from the source code, targeting especially academic courses and MOOCs.


\section{Results}

%Sum papers. Explain papers what they say about automated plagiarism. What is automated plagiarism? Use last names as reference. What are papers about? If math, explaining it open (using simple language). Limitations of other studies. "Combining these results would mean X". 

%Literature review

Machine learning in a source code plagiarism detection works like any other machine learning classification technique in another scenario. It is used to find recurring patterns from the data that could be used to separate it into individual parts. In source code plagiarism, this means finding features that could be used to identify the author and classify unseen data to most likely author. Unseen data in this sense means the source code from an unknown author, and the goal is to predict that which one of the possible authors wrote the unseen source code or is the unseen source code too similar to some other code.


Previous scenario is issued in \cite{bandara2011machine}, where Bandara and Wijayarathna are using attribute counting technique to develop a machine learning model to detect source code plagiarism. Attribute counting technique means, that the original source code is transformed into feature vector that contains only stylish metrics i.e. no data from the underlying structure. They report final accuracy as 86.65\%. Same idea of using style metrics from source code was researched earlier by Lange and Mancoridis \cite{lange2007using}. They use 18 various features and measure distribution differences of the metrics. Lange and Mancoridis report the final accuracy of their model as 75\%. Also Kothari \textit{et al.} take the coding style and character sequences as their features for the model \cite{kothari2007probabilistic}. They have six different style  and text distribution metrics which can represent the distribution of n-character patterns\footnote{E.g. using 2 character patterns slice the source code into two character slices.}. Their reported accuracy when using students source codes is for the pure style metrics 36\% and for the n-character 69\%. Elenbogen and Seliya build a data mining model based on six different style metrics in \cite{Elenbogen:2008:DOS:1295109.1295123}. They call these as a \textit{programming profiles} and these style metrics are described as elements that doesn't affect the functionality. Their accuracy using 83 programs and 12 students was 74.7\%. 

%more



Other studies use more structural analysis of the source code to form the features. In \cite{jadalla2008pde4java}, Jadalla and Elnagar describe the \textit{PDE4Java} detection engine to detect plagiarized Java-code. Authors use tokenization to build the needed format from the source code, that can be utilized for the N-Gram representation. This N-Gram representation is basically same as splitting the source code into slices of chosen length. Their data mining method reported suspicious source codes from eight various sized data sets about 6 out of 8 times in the same way as domain expert reported. The authors however didn't include the specific accuracy of the data mining model. Comparing to the study of Jadalla and Elnagar, even more structural-based analysis is done by Caliskan-Islam \textit{et al.} They use various code stylometries derived from abstract syntax trees to build the model that can de-anonymize programmers \cite{caliskan2015anonymizing}. Their model accuracy is 94\% when there were 1\ 600 possible authors and 98\% when there were 250 authors. Rosenblum \textit{et al.} researches that does programming style preserve after compilation \cite{rosenblum2011wrote}. They use style metrics for authorship identification, but this time style metrics from the compiled code as original source code might not always be available, making the style metrics more structural based. Their validation used 20 authors and the model reaches 77\% accuracy. Son \textit{et al.} propose a parse tree kernel to form the model based on structural features in \cite{Son:2013:APS:2508269.2508323} using 555 source codes. Their reported accuracy was 93\% same as using a human validator for the same task.

The collected papers seems to divide into two different categories: pure stylistic features \cite{bandara2011machine, lange2007using, kothari2007probabilistic, Elenbogen:2008:DOS:1295109.1295123} and structural based features \cite{jadalla2008pde4java,caliskan2015anonymizing,rosenblum2011wrote, Son:2013:APS:2508269.2508323}. As most stylistic metrics are based on easily collectable metrics from the original source code, structural approaches usually performs pre-processing to get the underlying features. Pre-processing is in many cases done by forming abstract syntax tree -representation which is not affected e.g. if names of the variables are changed.  

% more structural analysis

\subsection{Data}

The data sets used and their sizes in previous studies vary a little bit; source codes from college students, synthetic data sets, codes collected from open source projects and codes from open competitions e.g. \textit{Google Codejam}.  Most variance comes from the number of possible authors and the numbers of used source codes, but it doesn't seem to matter data-wise is the method structural or style based. The findings related to data set sizes are summed into following table, where \textit{size} refers to the total amount of files used. 

\begin{table}[ht]
\centering
\begin{tabular}{c|cccccccc}
\textbf{Attr./Paper} & \cite{bandara2011machine}   & \cite{kothari2007probabilistic}   & \cite{Elenbogen:2008:DOS:1295109.1295123}  & \cite{lange2007using}    & \cite{Son:2013:APS:2508269.2508323}   & \cite{caliskan2015anonymizing}    & \cite{jadalla2008pde4java}   & \cite{rosenblum2011wrote}   \\ \hline
\textbf{Size}        & 741 & 200 & 83 & 4068 & 555 & N/A  & 326 & 203 \\
\textbf{Authors}     & 10  & 8   & 12 & 20   & N/A & 1600 & N/A & 32 
\end{tabular}
\caption{Reported data sets used in papers.}
\label{table:data}
\end{table}

\noindent
In table \ref{table:data}, values are chosen only if they are explicitly told in the paper. If there have been multiple values or data sets, data set size have been selected as the one with student-related data, which are data sets collected from the courses. In three studies explicit values are not told and that is why they are marked as \textit{N/A}. For example in the study of Caliskan-Islam \textit{et al.}, their data set was collected from Google Codejam which explains their large amount of programmers\cite{caliskan2015anonymizing}. However they didn't explicitly tell how many source code files there were in total, but rather mentioned that there were around one to 17 code files per a contestant.

\subsection{Methodologies}

In this section, methodologies of collected studies are reviewed. Many style-related studies have similar methods, but their machine learning techniques or gathered feature sets differ. In structural-related, their methodologies can be very different but almost always utilizing the gained information from abstract syntax tree. 

\subsection{Feature extractions}

\subsection{Similarity detection}


\section{Analysis}
Reflection
\section{Discussion}

Main points/main ideas. Limitations

\section{Conclusion}

Sum up findings.


% --- References ---
%
% bibtex is used to generate the bibliography. The babplain style
% will generate numeric references (e.g. [1]) appropriate for theoretical
% computer science. If you need alphanumeric references (e.g [Tur90]), use
%
%\bibliographystyle{babalpha-lf}
%
% instead.

\bibliographystyle{babplain-lf}
\bibliography{references-fi}


% --- Appendices ---

% uncomment the following

% \newpage
% \appendix
% 
% \section{Esimerkkiliite}

\end{document}
